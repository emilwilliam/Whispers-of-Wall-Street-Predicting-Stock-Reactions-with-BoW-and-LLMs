{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Earnings Call Sentiment Analysis\n",
    "In this project, I’m diving into earnings call data from S&P 500 companies spanning 2015 to 2021 to build and compare sentiment measures. My goal is to see how well these measures—crafted using Bag-of-Words techniques and Large Language Models—can predict stock market reactions. I’ll load the data, clean the text, remove noise, and save my work for analysis. Here’s how I’ll approach it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Bringing in the Earnings Call Data\n",
    "Load the earnings call dataset along with presentation and Q&A texts from online sources. I’ll filter the Q&A to focus on answers, combine them with presentations, and create a full transcript column to work with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting counter for run time\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR-11-Carhart     60\n",
      "CAR-11-ff3         60\n",
      "CAR01-Carhart      60\n",
      "CAR01-ff3          60\n",
      "IV                101\n",
      "hvol              101\n",
      "IV_l1d            101\n",
      "IV_l2d            101\n",
      "IV_f1d            101\n",
      "actq              305\n",
      "rectq              40\n",
      "invtq              92\n",
      "lctq              305\n",
      "apq                42\n",
      "dpq               153\n",
      "cogsq               1\n",
      "oiadpq              3\n",
      "dlcq               60\n",
      "xintq             241\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Core packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import nltk\n",
    "\n",
    "### Load the Sample of Earnings Calls for the S&P500 from 2015 to 2021 with financials\n",
    "ECs = pd.read_csv(\"https://www.dropbox.com/scl/fi/2p7ahxroqj9pwf98ni5an/Sample_Calls.csv?rlkey=zfieicvz891u4e3z0aroeg0u7&dl=1\")\n",
    "\n",
    "### Load the Sample's Presentation texts\n",
    "Presentations = pd.read_feather(\"https://www.dropbox.com/scl/fi/uceh2xva5g4apbmt92cgt/Sample_Calls_Presentations.feather?rlkey=ln4nzsa4nenqyvm0pg2cur9sp&dl=1\")\n",
    "\n",
    "### Load the Q&A session textual data for the sample\n",
    "QAs = pd.read_feather(\"https://www.dropbox.com/scl/fi/iq4111nlmsykp2tzxk9xg/Sample_Calls_QA.feather?rlkey=xabjqmwhesx05jivrlfzkgj6m&dl=1\")\n",
    "\n",
    "### Filtering just the answers\n",
    "temp = QAs[QAs['QA'] == 'a']\n",
    "temp = temp.groupby('file_name')['QA_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "ECs['A'] = temp['QA_text']\n",
    "\n",
    "### Filtering just the questions\n",
    "temp = QAs[QAs['QA'] == 'q']\n",
    "temp = temp.groupby('file_name')['QA_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "ECs['Q'] = temp['QA_text']\n",
    "\n",
    "### Labeling questions and answers\n",
    "QAs['QA_text'] = np.where(QAs['QA'] == 'a', 'a: ' + QAs['QA_text'], 'q: ' + QAs['QA_text'])\n",
    "temp = QAs.groupby('file_name')['QA_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "ECs['QA'] = temp['QA_text']\n",
    "\n",
    "### Combining\n",
    "ECs['P'] = Presentations['presentation']\n",
    "ECs['PA'] = ECs['P'] + ' ' + ECs['A']\n",
    "\n",
    "### Deleting temp\n",
    "del temp\n",
    "\n",
    "### Turning niq into thousands\n",
    "ECs['niq'] = ECs['niq'] / 1000\n",
    "\n",
    "### Looking at missing data in our sample\n",
    "print(ECs.isnull().sum()[ECs.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2877, 50)\n",
      "(2817, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GVKEY</th>\n",
       "      <th>date_rdq</th>\n",
       "      <th>co_conm</th>\n",
       "      <th>file_name</th>\n",
       "      <th>CAR-11-Carhart</th>\n",
       "      <th>CAR-11-ff3</th>\n",
       "      <th>CAR01-Carhart</th>\n",
       "      <th>CAR01-ff3</th>\n",
       "      <th>IV</th>\n",
       "      <th>hvol</th>\n",
       "      <th>...</th>\n",
       "      <th>prccq</th>\n",
       "      <th>cshoq</th>\n",
       "      <th>dvpq</th>\n",
       "      <th>xintq</th>\n",
       "      <th>A</th>\n",
       "      <th>Q</th>\n",
       "      <th>QA</th>\n",
       "      <th>P</th>\n",
       "      <th>PA</th>\n",
       "      <th>SurpDec2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16101.0</td>\n",
       "      <td>2016-07-29 13:00:00+00:00</td>\n",
       "      <td>ABBVIE INC</td>\n",
       "      <td>Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.179151</td>\n",
       "      <td>0.129186</td>\n",
       "      <td>...</td>\n",
       "      <td>61.91</td>\n",
       "      <td>1628.542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>Sure. So in terms of sales on the Life Planner...</td>\n",
       "      <td>Hello, thank you. I just wanted to start with ...</td>\n",
       "      <td>q: Hello, thank you. I just wanted to start wi...</td>\n",
       "      <td>Good day and welcome to the Linear Technol...</td>\n",
       "      <td>Good day and welcome to the Linear Technol...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16101.0</td>\n",
       "      <td>2016-04-28 13:00:00+00:00</td>\n",
       "      <td>ABBVIE INC</td>\n",
       "      <td>Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...</td>\n",
       "      <td>0.026387</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.021770</td>\n",
       "      <td>0.289777</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>...</td>\n",
       "      <td>57.12</td>\n",
       "      <td>1617.359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>Ryan, this is Steve. We've long encouraged peo...</td>\n",
       "      <td>Hey, thanks. Good morning. I had a question ab...</td>\n",
       "      <td>q: Hey, thanks. Good morning. I had a question...</td>\n",
       "      <td>Welcome to Cerner Corporation's first quar...</td>\n",
       "      <td>Welcome to Cerner Corporation's first quar...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16101.0</td>\n",
       "      <td>2016-10-28 13:00:00+00:00</td>\n",
       "      <td>ABBVIE INC</td>\n",
       "      <td>Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...</td>\n",
       "      <td>-0.078668</td>\n",
       "      <td>-0.079290</td>\n",
       "      <td>-0.079290</td>\n",
       "      <td>-0.092594</td>\n",
       "      <td>0.253269</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>...</td>\n",
       "      <td>63.07</td>\n",
       "      <td>1624.908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>Jimmy, it's Rob. I'll take the first of those ...</td>\n",
       "      <td>Hi, good morning. I had a couple questions. Fi...</td>\n",
       "      <td>q: Hi, good morning. I had a couple questions....</td>\n",
       "      <td>Welcome to Cerner Corporation's second qua...</td>\n",
       "      <td>Welcome to Cerner Corporation's second qua...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16101.0</td>\n",
       "      <td>2017-01-27 14:00:00+00:00</td>\n",
       "      <td>ABBVIE INC</td>\n",
       "      <td>Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2017...</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>0.182080</td>\n",
       "      <td>0.145941</td>\n",
       "      <td>...</td>\n",
       "      <td>62.62</td>\n",
       "      <td>1592.513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>Great, Gregg, thank you for the questions. Dav...</td>\n",
       "      <td>Thank you. First on sola, how would you charac...</td>\n",
       "      <td>q: Thank you. First on sola, how would you cha...</td>\n",
       "      <td>Welcome to Cerner Corporation's third quar...</td>\n",
       "      <td>Welcome to Cerner Corporation's third quar...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16101.0</td>\n",
       "      <td>2017-04-27 13:00:00+00:00</td>\n",
       "      <td>ABBVIE INC</td>\n",
       "      <td>Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2017...</td>\n",
       "      <td>0.010397</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.192822</td>\n",
       "      <td>0.112189</td>\n",
       "      <td>...</td>\n",
       "      <td>65.16</td>\n",
       "      <td>1591.366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>Great, Mike, thanks for the question. For the ...</td>\n",
       "      <td>Hi, guys, this is Mike DiFiore in for Mark Sch...</td>\n",
       "      <td>q: Hi, guys, this is Mike DiFiore in for Mark ...</td>\n",
       "      <td>Welcome to Cerner Corporation's fourth qua...</td>\n",
       "      <td>Welcome to Cerner Corporation's fourth qua...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GVKEY                   date_rdq     co_conm  \\\n",
       "0  16101.0  2016-07-29 13:00:00+00:00  ABBVIE INC   \n",
       "1  16101.0  2016-04-28 13:00:00+00:00  ABBVIE INC   \n",
       "2  16101.0  2016-10-28 13:00:00+00:00  ABBVIE INC   \n",
       "3  16101.0  2017-01-27 14:00:00+00:00  ABBVIE INC   \n",
       "4  16101.0  2017-04-27 13:00:00+00:00  ABBVIE INC   \n",
       "\n",
       "                                           file_name  CAR-11-Carhart  \\\n",
       "0  Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...        0.011886   \n",
       "1  Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...        0.026387   \n",
       "2  Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2016...       -0.078668   \n",
       "3  Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2017...       -0.010152   \n",
       "4  Download ECC/SE/TRANSCRIPT/XMLStd/Archive/2017...        0.010397   \n",
       "\n",
       "   CAR-11-ff3  CAR01-Carhart  CAR01-ff3        IV      hvol  ...  prccq  \\\n",
       "0    0.014261       0.014261   0.021246  0.179151  0.129186  ...  61.91   \n",
       "1    0.023499       0.023499   0.021770  0.289777  0.114447  ...  57.12   \n",
       "2   -0.079290      -0.079290  -0.092594  0.253269  0.381002  ...  63.07   \n",
       "3   -0.000737      -0.000737  -0.005279  0.182080  0.145941  ...  62.62   \n",
       "4    0.010672       0.010672   0.012819  0.192822  0.112189  ...  65.16   \n",
       "\n",
       "      cshoq  dvpq  xintq                                                  A  \\\n",
       "0  1628.542   0.0  245.0  Sure. So in terms of sales on the Life Planner...   \n",
       "1  1617.359   0.0  215.0  Ryan, this is Steve. We've long encouraged peo...   \n",
       "2  1624.908   0.0  271.0  Jimmy, it's Rob. I'll take the first of those ...   \n",
       "3  1592.513   0.0  277.0  Great, Gregg, thank you for the questions. Dav...   \n",
       "4  1591.366   0.0  273.0  Great, Mike, thanks for the question. For the ...   \n",
       "\n",
       "                                                   Q  \\\n",
       "0  Hello, thank you. I just wanted to start with ...   \n",
       "1  Hey, thanks. Good morning. I had a question ab...   \n",
       "2  Hi, good morning. I had a couple questions. Fi...   \n",
       "3  Thank you. First on sola, how would you charac...   \n",
       "4  Hi, guys, this is Mike DiFiore in for Mark Sch...   \n",
       "\n",
       "                                                  QA  \\\n",
       "0  q: Hello, thank you. I just wanted to start wi...   \n",
       "1  q: Hey, thanks. Good morning. I had a question...   \n",
       "2  q: Hi, good morning. I had a couple questions....   \n",
       "3  q: Thank you. First on sola, how would you cha...   \n",
       "4  q: Hi, guys, this is Mike DiFiore in for Mark ...   \n",
       "\n",
       "                                                   P  \\\n",
       "0      Good day and welcome to the Linear Technol...   \n",
       "1      Welcome to Cerner Corporation's first quar...   \n",
       "2      Welcome to Cerner Corporation's second qua...   \n",
       "3      Welcome to Cerner Corporation's third quar...   \n",
       "4      Welcome to Cerner Corporation's fourth qua...   \n",
       "\n",
       "                                                  PA  SurpDec2  \n",
       "0      Good day and welcome to the Linear Technol...       4.0  \n",
       "1      Welcome to Cerner Corporation's first quar...       1.0  \n",
       "2      Welcome to Cerner Corporation's second qua...       1.0  \n",
       "3      Welcome to Cerner Corporation's third quar...       1.0  \n",
       "4      Welcome to Cerner Corporation's fourth qua...       1.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Priting the shape before removing missing data\n",
    "print(ECs.shape)\n",
    "\n",
    "### Removing any rows where our control variables and dependent variables are missing\n",
    "ECs = ECs.dropna(subset=['CAR-11-Carhart', 'niq', 'SurpDec', 'NUMUP', 'NUMDOWN'])\n",
    "\n",
    "### Resetting the index\n",
    "ECs = ECs.reset_index(drop=True)\n",
    "\n",
    "### Looking at the shape after removing missing data\n",
    "print(ECs.shape)\n",
    "\n",
    "### Creating a SurpDec squared variable\n",
    "ECs['SurpDec2'] = ECs['SurpDec'] ** 2\n",
    "ECs['SurpDec2'] = np.where(ECs['SurpDec'] > 0, ECs['SurpDec2'], ECs['SurpDec2'] * -1)\n",
    "\n",
    "### Looking at the first 5 rows\n",
    "ECs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns:\n",
    "\n",
    "### Identifiers\n",
    "- **GVKEY**: \"A unique company identifier used by Compustat.\"\n",
    "- **date_rdq**: \"The reporting date of the quarterly earnings or a related key event date.\"\n",
    "- **co_conm**: \"The company’s name in CRSP.\"\n",
    "\n",
    "### Earnings Call Columns\n",
    "- **file_name**: \"The identifier or filename of the earnings call transcript.\"\n",
    "- **CAR-11-Carhart**: \"Cumulative Abnormal Return over an event window using the Carhart 4-factor model.\" (-11 means -1 day to day 1, so one day before and day after report)\n",
    "- **CAR-11-ff3**: \"Cumulative Abnormal Return over an event window using the Fama-French 3-factor model.\"\n",
    "- **CAR01-Carhart**: \"Cumulative Abnormal Return (alternative window) using the Carhart 4-factor model.\" (01 means from day 0 to day 1, so on the day of the report)\n",
    "- **CAR01-ff3**: \"Cumulative Abnormal Return (alternative window) using the Fama-French 3-factor model.\"\n",
    "- **IV**: \"Implied volatility (often from options) reflecting expected future stock price volatility.\"\n",
    "- **hvol**: \"Historical volatility of the stock, based on past price movements.\"\n",
    "- **IV_l1d**: \"Implied volatility lagged by one day.\"\n",
    "- **IV_l2d**: \"Implied volatility lagged by two days.\"\n",
    "- **IV_f1d**: \"Implied volatility forecasted or measured one day forward.\"\n",
    "\n",
    "### I/B/E/S Columns\n",
    "- **NUMEST**: \"The number of analyst estimates contributing to the consensus.\"\n",
    "- **NUMUP**: \"The number of analysts who have revised their EPS estimates upward.\"\n",
    "- **NUMDOWN**: \"The number of analysts who have revised their EPS estimates downward.\"\n",
    "- **MEDEST**: \"The median of analyst EPS estimates.\"\n",
    "- **MEANEST**: \"The mean of analyst EPS estimates.\"\n",
    "- **ACTUAL**: \"The I/B/E/S standardized actual EPS figure, often adjusted for comparability.\"\n",
    "- **surp**: \"The earnings surprise, typically ACTUAL minus MEANEST.\"\n",
    "- **SurpDec**: \"A scaled or decimalized version of the earnings surprise.\"\n",
    "\n",
    "### Compustat Columns\n",
    "- **atq**: \"Total Assets (Quarterly)\"\n",
    "- **actq**: \"Current Assets (Quarterly)\"\n",
    "- **cheq**: \"Cash and Cash Equivalents (Quarterly)\"\n",
    "- **rectq**: \"Accounts Receivable (Quarterly)\"\n",
    "- **invtq**: \"Inventory (Quarterly)\"\n",
    "- **ltq**: \"Total Liabilities (Quarterly)\"\n",
    "- **lctq**: \"Current Liabilities (Quarterly)\"\n",
    "- **apq**: \"Accounts Payable (Quarterly)\"\n",
    "- **ceqq**: \"Total Equity (Quarterly)\"\n",
    "- **seqq**: \"Common Equity (Quarterly)\"\n",
    "- **capxy**: \"Capital Expenditures (Note: 'capxy' is annual by default, quarterly approximations derived from segments)\"\n",
    "- **dpq**: \"Depreciation and Amortization (Quarterly)\"\n",
    "- **saleq**: \"Revenue (Quarterly)\"\n",
    "- **cogsq**: \"Cost of Goods Sold (Quarterly)\"\n",
    "- **oiadpq**: \"Operating Income (Quarterly)\"\n",
    "- **niq**: \"Net Income (Quarterly)\"\n",
    "- **epspxq**: \"Basic Earnings Per Share (Quarterly)\"\n",
    "- **epspiq**: \"Diluted Earnings Per Share (Quarterly)\"\n",
    "- **dlttq**: \"Long-Term Debt (Quarterly)\"\n",
    "- **dlcq**: \"Debt in Current Liabilities (Quarterly)\"\n",
    "- **prccq**: \"Price Close - Fiscal Quarter\"\n",
    "- **cshoq**: \"Common Shares Outstanding (Quarterly)\"\n",
    "- **dvpq**: \"Dividends Paid (Quarterly)\"\n",
    "- **xintq**: \"Interest Expense (Quarterly)\"\n",
    "\n",
    "### Text Variables \n",
    "*(All tokenized, stemmed, stopwords removed and punctuations removed)*\n",
    "- **P**: \"Presentation text\"\n",
    "- **A**: \"All answers by management to questions\"\n",
    "- **PA**: \"Presentation text + Answer text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Picking a smaller sample\n",
    "Now we will shorten the data down while we construct the code. We will put this code in comments (#) when running the code on the entire dataset when all is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Getting a sample of the data\n",
    "# ECs = ECs.sample(100, random_state=123)\n",
    "\n",
    "# ### Resetting the index\n",
    "# ECs = ECs.reset_index(drop=True)\n",
    "\n",
    "# #Looking at the shape of the data\n",
    "# print(ECs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing the Text\n",
    "At this point, I’ll clean up the text data. I’ll tokenize it, remove stopwords, and apply stemming to simplify the words in the presentations, answers, and full transcripts, making them ready for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2817/2817 [00:57<00:00, 49.32it/s] \n",
      "100%|██████████| 2817/2817 [01:00<00:00, 46.39it/s]\n"
     ]
    }
   ],
   "source": [
    "### Importing packages for cleaning\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "### Adding a progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "### Initialize the stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "### Define a function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "    - Tokenizing\n",
    "    - Converting to lowercase\n",
    "    - Removing non-alphabetic tokens\n",
    "    - Removing stopwords\n",
    "    - Applying stemming\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Check if the input is a string\n",
    "        return []\n",
    "    tokens = word_tokenize(text)                                  # Tokenize the text\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Lowercase & keep alphabetic tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [stemmer.stem(word) for word in tokens]              # Apply stemming\n",
    "    return tokens\n",
    "\n",
    "### Apply the cleaning function to the specified columns\n",
    "ECs['P_cleaned'] = ECs['P'].progress_apply(clean_text)\n",
    "ECs['A_cleaned'] = ECs['A'].progress_apply(clean_text)\n",
    "ECs['PA_cleaned'] = ECs['P_cleaned'] + ECs['A_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Removing Overly Common Words\n",
    "Now, I’ll identify and remove words that show up in over 75% of the documents. These are too frequent to carry unique sentiment, so stripping them out will sharpen my focus on what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go', 2817), ('thank', 2817), ('also', 2817), ('see', 2817), ('quarter', 2817), ('us', 2817), ('look', 2817), ('year', 2817), ('continu', 2817), ('forward', 2817), ('busi', 2816), ('good', 2816), ('call', 2816), ('well', 2816), ('first', 2815), ('time', 2815), ('would', 2815), ('last', 2814), ('like', 2814), ('oper', 2814), ('expect', 2814), ('think', 2813), ('result', 2813), ('question', 2811), ('point', 2811), ('one', 2810), ('today', 2810), ('make', 2808), ('get', 2808), ('growth', 2806), ('realli', 2806), ('new', 2806), ('take', 2804), ('includ', 2803), ('work', 2802), ('end', 2802), ('million', 2802), ('strong', 2800), ('increas', 2799), ('earn', 2799), ('product', 2799), ('market', 2798), ('term', 2798), ('turn', 2795), ('share', 2795), ('start', 2795), ('posit', 2793), ('impact', 2793), ('come', 2790), ('rate', 2786), ('financi', 2785), ('back', 2785), ('compani', 2782), ('perform', 2781), ('cost', 2781), ('thing', 2779), ('right', 2778), ('invest', 2778), ('base', 2777), ('second', 2776), ('actual', 2775), ('addit', 2773), ('differ', 2770), ('ye', 2769), ('part', 2765), ('number', 2764), ('provid', 2763), ('line', 2762), ('lot', 2761), ('improv', 2759), ('talk', 2759), ('gener', 2757), ('way', 2757), ('say', 2754), ('move', 2754), ('next', 2750), ('basi', 2750), ('relat', 2749), ('plan', 2748), ('across', 2747), ('opportun', 2747), ('want', 2745), ('higher', 2744), ('much', 2743), ('chang', 2743), ('level', 2741), ('around', 2739), ('still', 2734), ('statement', 2733), ('cash', 2732), ('remain', 2732), ('capit', 2729), ('know', 2728), ('margin', 2728), ('grow', 2724), ('may', 2723), ('pleas', 2722), ('billion', 2721), ('full', 2720), ('said', 2719), ('littl', 2716), ('let', 2707), ('total', 2698), ('sale', 2696), ('manag', 2694), ('could', 2692), ('custom', 2689), ('long', 2688), ('give', 2686), ('need', 2684), ('day', 2684), ('benefit', 2680), ('bit', 2678), ('execut', 2676), ('high', 2675), ('use', 2674), ('drive', 2673), ('deliv', 2673), ('lower', 2672), ('close', 2668), ('signific', 2666), ('even', 2666), ('believ', 2664), ('mention', 2664), ('better', 2661), ('futur', 2661), ('price', 2660), ('sure', 2656), ('current', 2656), ('help', 2655), ('team', 2654), ('guidanc', 2649), ('discuss', 2647), ('made', 2645), ('flow', 2645), ('overal', 2643), ('compar', 2637), ('per', 2637), ('balanc', 2636), ('valu', 2634), ('month', 2634), ('put', 2633), ('great', 2631), ('begin', 2627), ('driven', 2620), ('rang', 2619), ('import', 2616), ('focu', 2607), ('return', 2595), ('focus', 2595), ('mani', 2594), ('net', 2591), ('everyon', 2591), ('materi', 2586), ('investor', 2585), ('third', 2583), ('seen', 2580), ('larg', 2579), ('revenu', 2578), ('releas', 2574), ('half', 2571), ('activ', 2570), ('effect', 2567), ('build', 2563), ('industri', 2560), ('expens', 2558), ('adjust', 2558), ('abl', 2557), ('due', 2556), ('join', 2554), ('recent', 2554), ('servic', 2554), ('support', 2553), ('record', 2552), ('develop', 2550), ('interest', 2548), ('approxim', 2547), ('coupl', 2543), ('got', 2537), ('confer', 2534), ('report', 2530), ('offset', 2524), ('earlier', 2524), ('period', 2520), ('mean', 2517), ('program', 2514), ('reflect', 2514), ('strategi', 2512), ('earli', 2510), ('given', 2507), ('saw', 2505), ('progress', 2501), ('area', 2500), ('detail', 2498), ('welcom', 2493), ('initi', 2492), ('comment', 2484), ('done', 2480), ('key', 2475), ('pretti', 2474), ('ahead', 2464), ('factor', 2462), ('avail', 2462), ('specif', 2460), ('fourth', 2459), ('place', 2459), ('tax', 2459), ('add', 2455), ('feel', 2453), ('success', 2452), ('open', 2448), ('top', 2448), ('risk', 2445), ('within', 2441), ('obvious', 2438), ('consist', 2435), ('portfolio', 2430), ('probabl', 2427), ('past', 2426), ('less', 2423), ('low', 2415), ('anoth', 2414), ('updat', 2412), ('best', 2409), ('environ', 2409), ('versu', 2409), ('big', 2405), ('process', 2404), ('kind', 2400), ('confid', 2399), ('combin', 2393), ('prior', 2391), ('mix', 2389), ('spend', 2387), ('note', 2385), ('websit', 2379), ('abil', 2376), ('demand', 2372), ('sever', 2369), ('announc', 2367), ('run', 2367), ('remind', 2364), ('peopl', 2364), ('ago', 2364), ('expand', 2363), ('digit', 2363), ('complet', 2362), ('alway', 2362), ('trend', 2361), ('bring', 2358), ('gaap', 2355), ('tri', 2355), ('declin', 2355), ('segment', 2355), ('file', 2354), ('fact', 2352), ('primarili', 2340), ('presid', 2337), ('creat', 2335), ('happen', 2333), ('alreadi', 2329), ('side', 2328), ('project', 2327), ('technolog', 2327), ('inform', 2324), ('measur', 2322), ('final', 2321), ('lead', 2321), ('everi', 2319), ('target', 2319), ('incom', 2316), ('reduc', 2314), ('major', 2309), ('set', 2308), ('follow', 2306), ('profit', 2298), ('someth', 2298), ('remark', 2296), ('certainli', 2296), ('competit', 2291), ('offic', 2287), ('effici', 2284), ('leverag', 2283), ('issu', 2282), ('keep', 2275), ('model', 2273), ('acquisit', 2266), ('sharehold', 2263), ('non', 2262), ('rel', 2261), ('capabl', 2252), ('allow', 2249), ('organ', 2244), ('excit', 2242), ('anticip', 2241), ('world', 2239), ('volum', 2238), ('cours', 2237), ('sinc', 2230), ('achiev', 2230), ('exampl', 2230), ('commit', 2229), ('outlook', 2227), ('acceler', 2223), ('challeng', 2218), ('track', 2215), ('view', 2211), ('account', 2209), ('particularli', 2207), ('system', 2205), ('offer', 2201), ('intern', 2200), ('reason', 2199), ('present', 2192), ('item', 2186), ('gain', 2182), ('strength', 2182), ('answer', 2181), ('morn', 2175), ('averag', 2173), ('amount', 2166), ('contribut', 2161), ('strateg', 2159), ('toward', 2157), ('week', 2156), ('show', 2153), ('refer', 2152), ('global', 2144), ('potenti', 2143), ('platform', 2142), ('prepar', 2138), ('togeth', 2134), ('ad', 2123), ('data', 2121), ('asset', 2117), ('highlight', 2117), ('annual', 2115), ('core', 2106), ('certain', 2105), ('play', 2104), ('effort', 2104), ('state', 2094), ('order', 2092), ('approach', 2087), ('experi', 2084), ('consum', 2083), ('hard', 2083), ('far', 2082), ('quit', 2081), ('exclud', 2081), ('chief', 2075), ('flat', 2065), ('repres', 2064), ('solid', 2064), ('singl', 2064), ('case', 2059), ('ep', 2054), ('integr', 2053), ('expans', 2038), ('gross', 2038), ('mayb', 2022), ('uncertainti', 2022), ('free', 2020), ('review', 2019), ('reduct', 2018), ('perspect', 2013), ('caus', 2013), ('innov', 2011), ('normal', 2011), ('debt', 2010), ('sheet', 2006), ('understand', 2004), ('unit', 2002), ('slightli', 2000), ('date', 1999), ('maintain', 1997), ('suppli', 1992), ('along', 1989), ('commun', 1987), ('significantli', 1987), ('launch', 1986), ('instruct', 1986), ('grew', 1985), ('partner', 1975), ('dividend', 1973), ('real', 1972), ('shift', 1972), ('beyond', 1971), ('meet', 1970), ('driver', 1969), ('throughout', 1969), ('hope', 1965), ('deal', 1965), ('becom', 1957), ('associ', 1953), ('direct', 1952), ('estim', 1952), ('reconcili', 1952), ('momentum', 1949), ('sell', 1943), ('thought', 1942), ('small', 1937), ('requir', 1930), ('structur', 1926), ('similar', 1921), ('exist', 1921), ('increment', 1920), ('anyth', 1915), ('clearli', 1914), ('clear', 1912), ('sustain', 1909), ('tell', 1886), ('inventori', 1885), ('enhanc', 1884), ('action', 1880), ('later', 1875), ('season', 1873), ('doubl', 1870), ('step', 1869), ('neg', 1869), ('whether', 1868), ('longer', 1866), ('repurchas', 1863), ('howev', 1860), ('commerci', 1860), ('decis', 1854), ('indic', 1851), ('job', 1840), ('capac', 1835), ('favor', 1833), ('enabl', 1826), ('america', 1824), ('pay', 1822), ('solut', 1822), ('regard', 1820), ('receiv', 1815), ('region', 1815), ('ongo', 1813), ('histor', 1812), ('ask', 1811), ('assum', 1811), ('sec', 1810), ('address', 1810), ('find', 1810), ('space', 1800), ('serv', 1800), ('previous', 1799), ('might', 1794), ('secur', 1794), ('board', 1794), ('form', 1793), ('short', 1790), ('event', 1788), ('save', 1787), ('front', 1782), ('goal', 1782), ('categori', 1778), ('dollar', 1777), ('distribut', 1776), ('vice', 1774), ('percentag', 1771), ('fulli', 1769), ('yet', 1766), ('respect', 1763), ('decreas', 1762), ('brand', 1756), ('greater', 1754), ('qualiti', 1753), ('headwind', 1752), ('natur', 1749), ('rest', 1748), ('particular', 1746), ('group', 1745), ('design', 1744), ('advantag', 1740), ('econom', 1737), ('north', 1736), ('control', 1736), ('power', 1735), ('care', 1734), ('press', 1730), ('corpor', 1729), ('stronger', 1723), ('pipelin', 1719), ('transact', 1718), ('contract', 1715), ('speak', 1708), ('quarterli', 1706), ('largest', 1702), ('came', 1702), ('countri', 1700), ('everyth', 1698), ('stay', 1698), ('partial', 1692), ('implement', 1690), ('though', 1689), ('retail', 1686), ('depend', 1680), ('uniqu', 1675), ('almost', 1674), ('encourag', 1673), ('advanc', 1671), ('multipl', 1668), ('ceo', 1663), ('forecast', 1662), ('entir', 1660), ('cycl', 1660), ('slide', 1656), ('type', 1655), ('whole', 1655), ('reach', 1651), ('hit', 1650), ('center', 1650), ('pressur', 1648), ('purchas', 1636), ('post', 1635), ('near', 1635), ('win', 1633), ('stock', 1632), ('demonstr', 1632), ('optim', 1631), ('buy', 1627), ('dilut', 1625), ('absolut', 1624), ('employe', 1624), ('okay', 1624), ('despit', 1617), ('least', 1616), ('convers', 1616), ('recogn', 1613), ('mid', 1613), ('scale', 1611), ('engag', 1610), ('channel', 1608), ('cover', 1607), ('europ', 1607), ('hand', 1603), ('sens', 1594), ('experienc', 1592), ('credit', 1590), ('deploy', 1590), ('sequenti', 1590), ('outstand', 1590), ('without', 1589), ('ca', 1586), ('prioriti', 1585), ('roughli', 1584), ('especi', 1582), ('access', 1581), ('piec', 1577), ('respons', 1574), ('fund', 1574), ('quickli', 1574), ('transit', 1574), ('possibl', 1574), ('facil', 1566), ('happi', 1564), ('two', 1563), ('origin', 1561), ('util', 1556), ('taken', 1555), ('size', 1555), ('hold', 1553), ('differenti', 1548), ('basic', 1543), ('plu', 1543), ('china', 1542), ('ramp', 1540), ('built', 1539), ('goe', 1537), ('exactli', 1533), ('sort', 1531), ('leadership', 1527), ('nearli', 1526), ('relationship', 1524), ('typic', 1521), ('meaning', 1517), ('exceed', 1515), ('outsid', 1512), ('difficult', 1510), ('condit', 1509), ('metric', 1503), ('chain', 1503), ('either', 1502), ('disciplin', 1502), ('critic', 1501), ('although', 1495), ('network', 1495), ('book', 1494), ('broad', 1493), ('loss', 1488), ('test', 1488), ('everybodi', 1487), ('subject', 1486), ('portion', 1481), ('other', 1480), ('dynam', 1478), ('strengthen', 1477), ('realiz', 1473), ('previou', 1472), ('rememb', 1471), ('limit', 1471), ('nice', 1469), ('definit', 1467), ('faster', 1467), ('resourc', 1466), ('attract', 1466), ('particip', 1466), ('ever', 1465), ('marketplac', 1462), ('appreci', 1458), ('excel', 1457), ('importantli', 1455), ('manufactur', 1454), ('connect', 1454), ('mind', 1450), ('head', 1448), ('affect', 1445), ('flexibl', 1444), ('led', 1443), ('roll', 1443), ('consid', 1435), ('currenc', 1434), ('took', 1432), ('march', 1430), ('charg', 1429), ('home', 1427), ('readi', 1424), ('robust', 1423), ('onlin', 1423), ('found', 1422), ('alloc', 1418), ('infrastructur', 1418), ('learn', 1417), ('pace', 1415), ('ensur', 1413), ('schedul', 1412), ('bottom', 1411), ('late', 1410), ('health', 1408), ('produc', 1404), ('variou', 1404), ('essenti', 1403), ('compon', 1401), ('behind', 1397), ('assumpt', 1396), ('except', 1396), ('extrem', 1395), ('larger', 1394), ('introduc', 1394), ('matter', 1392), ('live', 1392), ('mark', 1390), ('ultim', 1390), ('extend', 1389), ('went', 1387), ('contain', 1385), ('describ', 1383), ('exchang', 1379), ('trade', 1379), ('consolid', 1377), ('appropri', 1372), ('deliveri', 1371), ('leader', 1371), ('transform', 1367), ('face', 1360), ('predict', 1353), ('highli', 1351), ('ratio', 1347), ('equiti', 1338), ('standpoint', 1337), ('underli', 1337), ('biggest', 1337), ('enter', 1337), ('option', 1335), ('count', 1334), ('middl', 1334), ('sign', 1330), ('situat', 1327), ('decemb', 1326), ('wo', 1326), ('comfort', 1323), ('noth', 1322), ('govern', 1319), ('guid', 1315), ('align', 1315), ('occur', 1315), ('moment', 1315), ('payment', 1314), ('recoveri', 1310), ('stand', 1310), ('substanti', 1310), ('ga', 1309), ('color', 1307), ('proud', 1306), ('standard', 1304), ('applic', 1301), ('away', 1299), ('pick', 1298), ('chairman', 1298), ('rais', 1294), ('tremend', 1294), ('highest', 1293), ('healthi', 1291), ('news', 1286), ('life', 1281), ('june', 1281), ('smaller', 1274), ('k', 1272), ('fiscal', 1267), ('emerg', 1264), ('local', 1259), ('agreement', 1256), ('stage', 1252), ('energi', 1251), ('acquir', 1245), ('januari', 1244), ('partnership', 1239), ('locat', 1238), ('asia', 1233), ('capex', 1233), ('upon', 1231), ('section', 1231), ('concern', 1230), ('pull', 1230), ('correct', 1225), ('competitor', 1225), ('person', 1224), ('liquid', 1219), ('safe', 1219), ('push', 1214), ('engin', 1213), ('true', 1212), ('phase', 1208), ('finish', 1208), ('function', 1206), ('conclud', 1204), ('guess', 1204), ('aggress', 1203), ('comparison', 1199), ('fundament', 1195), ('slow', 1194), ('store', 1192), ('sourc', 1190), ('directli', 1189), ('object', 1187), ('stori', 1186), ('summari', 1183), ('evalu', 1182), ('comp', 1182), ('adopt', 1181), ('approv', 1180), ('profil', 1180), ('money', 1179), ('els', 1178), ('bigger', 1178), ('fall', 1177), ('septemb', 1176), ('construct', 1174), ('select', 1173), ('drove', 1173), ('broader', 1172), ('primari', 1172), ('expenditur', 1169), ('april', 1168), ('paid', 1165), ('captur', 1165), ('hear', 1164), ('stabl', 1163), ('tool', 1160), ('frankli', 1159), ('public', 1155), ('modest', 1154), ('histori', 1154), ('financ', 1153), ('name', 1152), ('compens', 1148), ('protect', 1147), ('element', 1147), ('qa', 1146), ('yield', 1145), ('fairli', 1144), ('volatil', 1143), ('never', 1142), ('fair', 1141), ('economi', 1140), ('john', 1140), ('remaind', 1136), ('tradit', 1134), ('tend', 1133), ('evolv', 1129), ('stabil', 1126), ('identifi', 1122), ('class', 1121), ('foreign', 1119), ('footprint', 1118), ('role', 1117), ('februari', 1116), ('premium', 1114), ('compet', 1113), ('ebitda', 1110), ('sga', 1109), ('closer', 1108), ('involv', 1107), ('collect', 1104), ('promot', 1103), ('foundat', 1103), ('optimist', 1101), ('three', 1100), ('recal', 1099), ('mobil', 1096), ('path', 1095), ('oblig', 1084), ('gave', 1082), ('individu', 1082), ('sold', 1081), ('safeti', 1081), ('watch', 1079), ('enough', 1078), ('octob', 1077), ('excess', 1075), ('fix', 1074), ('heard', 1074), ('incred', 1073), ('regulatori', 1069), ('juli', 1067), ('equip', 1066), ('cfo', 1064), ('bill', 1063), ('visibl', 1062), ('nation', 1060), ('establish', 1057), ('somewhat', 1055), ('enterpris', 1052), ('therefor', 1044), ('plant', 1043), ('author', 1043), ('respond', 1042), ('fuel', 1038), ('complex', 1036), ('outcom', 1034), ('american', 1030), ('touch', 1025), ('member', 1024), ('gone', 1022), ('reform', 1018), ('extent', 1017), ('matur', 1013), ('began', 1012), ('hour', 1012), ('site', 1010), ('sorri', 1010), ('huge', 1009), ('practic', 1007), ('act', 1003), ('pass', 1001), ('intend', 999), ('purpos', 999), ('disrupt', 999), ('synergi', 995), ('speed', 993), ('necessarili', 993), ('field', 990), ('exit', 989), ('moder', 987), ('minut', 985), ('appli', 983), ('light', 981), ('translat', 978), ('upsid', 977), ('mike', 976), ('variabl', 973), ('afternoon', 970), ('senior', 970), ('replac', 969), ('fast', 969), ('dramat', 969), ('break', 967), ('budget', 965), ('insight', 964), ('leav', 963), ('deep', 963), ('tough', 963), ('sit', 963), ('forc', 961), ('spent', 961), ('fit', 959), ('otherwis', 958), ('restructur', 958), ('amort', 957), ('decad', 955), ('rather', 954), ('determin', 953), ('dedic', 953), ('upgrad', 952), ('privat', 949), ('shape', 949), ('weight', 946), ('ship', 946), ('oil', 946), ('common', 944), ('geographi', 941), ('wide', 940), ('awar', 938), ('brought', 938), ('among', 938), ('cut', 938), ('geograph', 937), ('defin', 936), ('sometim', 934), ('content', 933), ('attribut', 932), ('fee', 930), ('drop', 929), ('separ', 929), ('train', 929), ('outlin', 928), ('list', 927), ('walk', 925), ('host', 924), ('monitor', 923), ('famili', 923), ('steadi', 919), ('context', 917), ('special', 913), ('read', 913), ('delay', 913), ('idea', 912), ('talent', 910), ('carri', 906), ('packag', 905), ('intent', 902), ('recov', 901), ('constant', 901), ('domest', 898), ('spread', 896), ('cultur', 892), ('renew', 891), ('novemb', 890), ('impress', 889), ('commiss', 886), ('worth', 884), ('spot', 883), ('august', 881), ('consecut', 881), ('client', 879), ('supplement', 879), ('legaci', 879), ('instal', 878), ('accomplish', 877), ('reserv', 877), ('wait', 875), ('franchis', 875), ('choic', 874), ('mostli', 873), ('collabor', 872), ('problem', 871), ('simpli', 868), ('figur', 867), ('usual', 866), ('listen', 863), ('sector', 859), ('aspect', 859), ('midpoint', 857), ('inflat', 857), ('necessari', 856), ('assess', 856), ('extens', 855), ('featur', 854), ('game', 854), ('commod', 854), ('analyt', 853), ('consider', 848), ('softwar', 847), ('penetr', 845), ('properti', 844), ('pursu', 843), ('weak', 842), ('seem', 842), ('prospect', 841), ('movement', 840), ('buyback', 839), ('room', 839), ('held', 839), ('weather', 837), ('david', 836), ('calendar', 835), ('soon', 834), ('soft', 833), ('incent', 832), ('surpris', 832), ('virtual', 832), ('backlog', 830), ('et', 829), ('regul', 827), ('equal', 826), ('notabl', 824), ('express', 823), ('percent', 823), ('defer', 822)]\n",
      "Removing 639 words that appear in more than 50.0% of the documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2817/2817 [00:08<00:00, 333.51it/s]\n",
      "100%|██████████| 2817/2817 [00:07<00:00, 364.44it/s]\n"
     ]
    }
   ],
   "source": [
    "### Calculate the document frequency for each unique word\n",
    "from collections import Counter\n",
    "doc_freq = Counter()\n",
    "\n",
    "### Iterate over each presentation, clean the text, and update the document frequency\n",
    "for presentation in ECs.PA_cleaned:\n",
    "    cleaned_words = set(presentation)\n",
    "    doc_freq.update(cleaned_words)\n",
    "\n",
    "### Retrieve and display the top 'top_n' most common words by document frequency\n",
    "top_common_words = doc_freq.most_common(1000)\n",
    "print(top_common_words)\n",
    "\n",
    "### Removing all words that appear in more than 75% of the documents\n",
    "threshold = 0.50\n",
    "words_to_remove = [word for word, count in top_common_words if count > threshold * len(ECs)]\n",
    "print(f\"Removing {len(words_to_remove)} words that appear in more than {threshold*100}% of the documents.\")\n",
    "\n",
    "### Removing the words in words_to_remove\n",
    "def remove_common_words(text):\n",
    "    return [word for word in text if word not in words_to_remove]\n",
    "ECs['P_cleaned'] = ECs['P_cleaned'].progress_apply(remove_common_words)\n",
    "ECs['A_cleaned'] = ECs['A_cleaned'].progress_apply(remove_common_words)\n",
    "ECs['PA_cleaned'] = ECs['P_cleaned'] + ECs['A_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Turning Tokens Back into Strings\n",
    "After cleaning and filtering, I’ll join the tokenized words back into strings. Since the date is saved in SQL, it need to be strings and not lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2817/2817 [00:00<00:00, 56990.63it/s]\n",
      "100%|██████████| 2817/2817 [00:00<00:00, 24251.00it/s]\n",
      "100%|██████████| 2817/2817 [00:00<00:00, 40253.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>A</th>\n",
       "      <th>PA</th>\n",
       "      <th>P_cleaned</th>\n",
       "      <th>A_cleaned</th>\n",
       "      <th>PA_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good day and welcome to the Linear Technol...</td>\n",
       "      <td>Sure. So in terms of sales on the Life Planner...</td>\n",
       "      <td>Good day and welcome to the Linear Technol...</td>\n",
       "      <td>linear fiscal zerio financ sir linear bob swan...</td>\n",
       "      <td>life planner gibraltar life planner life plann...</td>\n",
       "      <td>linear fiscal zerio financ sir linear bob swan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Cerner Corporation's first quar...</td>\n",
       "      <td>Ryan, this is Steve. We've long encouraged peo...</td>\n",
       "      <td>Welcome to Cerner Corporation's first quar...</td>\n",
       "      <td>cerner variou constitut prospect health client...</td>\n",
       "      <td>ryan steve roa annuiti gradual pdi fee bip roa...</td>\n",
       "      <td>cerner variou constitut prospect health client...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welcome to Cerner Corporation's second qua...</td>\n",
       "      <td>Jimmy, it's Rob. I'll take the first of those ...</td>\n",
       "      <td>Welcome to Cerner Corporation's second qua...</td>\n",
       "      <td>cerner august variou constitut prospect health...</td>\n",
       "      <td>jimmi rob steve annuiti stori minut recal thre...</td>\n",
       "      <td>cerner august variou constitut prospect health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Welcome to Cerner Corporation's third quar...</td>\n",
       "      <td>Great, Gregg, thank you for the questions. Dav...</td>\n",
       "      <td>Welcome to Cerner Corporation's third quar...</td>\n",
       "      <td>cerner novemb variou constitut prospect health...</td>\n",
       "      <td>gregg dave regulatori interact solanezumab jan...</td>\n",
       "      <td>cerner novemb variou constitut prospect health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Welcome to Cerner Corporation's fourth qua...</td>\n",
       "      <td>Great, Mike, thanks for the question. For the ...</td>\n",
       "      <td>Welcome to Cerner Corporation's fourth qua...</td>\n",
       "      <td>cerner februari variou constitut prospect conc...</td>\n",
       "      <td>mike solanezumab scenario function cognit comp...</td>\n",
       "      <td>cerner februari variou constitut prospect conc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   P  \\\n",
       "0      Good day and welcome to the Linear Technol...   \n",
       "1      Welcome to Cerner Corporation's first quar...   \n",
       "2      Welcome to Cerner Corporation's second qua...   \n",
       "3      Welcome to Cerner Corporation's third quar...   \n",
       "4      Welcome to Cerner Corporation's fourth qua...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  Sure. So in terms of sales on the Life Planner...   \n",
       "1  Ryan, this is Steve. We've long encouraged peo...   \n",
       "2  Jimmy, it's Rob. I'll take the first of those ...   \n",
       "3  Great, Gregg, thank you for the questions. Dav...   \n",
       "4  Great, Mike, thanks for the question. For the ...   \n",
       "\n",
       "                                                  PA  \\\n",
       "0      Good day and welcome to the Linear Technol...   \n",
       "1      Welcome to Cerner Corporation's first quar...   \n",
       "2      Welcome to Cerner Corporation's second qua...   \n",
       "3      Welcome to Cerner Corporation's third quar...   \n",
       "4      Welcome to Cerner Corporation's fourth qua...   \n",
       "\n",
       "                                           P_cleaned  \\\n",
       "0  linear fiscal zerio financ sir linear bob swan...   \n",
       "1  cerner variou constitut prospect health client...   \n",
       "2  cerner august variou constitut prospect health...   \n",
       "3  cerner novemb variou constitut prospect health...   \n",
       "4  cerner februari variou constitut prospect conc...   \n",
       "\n",
       "                                           A_cleaned  \\\n",
       "0  life planner gibraltar life planner life plann...   \n",
       "1  ryan steve roa annuiti gradual pdi fee bip roa...   \n",
       "2  jimmi rob steve annuiti stori minut recal thre...   \n",
       "3  gregg dave regulatori interact solanezumab jan...   \n",
       "4  mike solanezumab scenario function cognit comp...   \n",
       "\n",
       "                                          PA_cleaned  \n",
       "0  linear fiscal zerio financ sir linear bob swan...  \n",
       "1  cerner variou constitut prospect health client...  \n",
       "2  cerner august variou constitut prospect health...  \n",
       "3  cerner novemb variou constitut prospect health...  \n",
       "4  cerner februari variou constitut prospect conc...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Turning the tokens back into a string\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "ECs['P_cleaned'] = ECs['P_cleaned'].progress_apply(tokens_to_string)\n",
    "ECs['A_cleaned'] = ECs['A_cleaned'].progress_apply(tokens_to_string)\n",
    "ECs['PA_cleaned'] = ECs['PA_cleaned'].progress_apply(tokens_to_string)\n",
    "\n",
    "### Display the first rows of the cleaned data\n",
    "ECs[['P', 'A', 'PA', 'P_cleaned', 'A_cleaned', 'PA_cleaned']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Storing the Processed Data\n",
    "Finally, I’ll save my processed dataset into a SQLite database. This keeps everything organized and secure, ready for the next phase of sentiment analysis and prediction. We also keep a back-up file saved in each code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the data to a SQLite database\n",
    "conn = sql.connect('data.db')\n",
    "ECs.to_sql('ECs1', conn, if_exists='replace')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 155.57 seconds\n"
     ]
    }
   ],
   "source": [
    "### Stopping the timer\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
